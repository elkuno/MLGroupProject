{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X_train_transformed = pd.read_csv('CSV/pre-processed/full_X_train_transformed')\n",
    "full_y_train = pd.read_csv('CSV/pre-processed/full_y_train')\n",
    "full_y_train = full_y_train.drop(columns=['id'])\n",
    "full_y_train = full_y_train.iloc[:, 0]\n",
    "\n",
    "\n",
    "\n",
    "full_X_test_transformed = pd.read_csv('CSV/pre-processed/full_X_test_transformed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only the image data from the full_X_train_transformed DataFrame\n",
    "image_data_train = full_X_train_transformed.iloc[:, -40000:]\n",
    "image_data_test = full_X_test_transformed.iloc[:, -40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_train.columns = range(40000)\n",
    "image_data_test.columns = range(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_train = image_data_train.values.reshape(-1, 200, 200)\n",
    "image_data_test = image_data_test.values.reshape(-1, 200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 200, 200)\n",
      "(594, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "print(image_data_train.shape)\n",
    "print(image_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (792, 200, 200)\n",
      "Validation set size: (198, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(full_y_train)\n",
    "\n",
    "# Splitting 20% of the training data as a validation set\n",
    "X_train, X_val, y_train_encoded_split, y_val_encoded_split = train_test_split(\n",
    "    image_data_train, y_train_encoded, test_size=0.2, stratify=y_train_encoded, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Calculate the size of the flattened output after the conv and pooling layers\n",
    "        flattened_size = 64 * 50 * 50\n",
    "        \n",
    "        self.fc1 = nn.Linear(flattened_size, 512)\n",
    "        self.fc2 = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 50 * 50)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntermediateCNN(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(IntermediateCNN, self).__init__()\n",
    "\n",
    "        # First Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(64 * 50 * 50, 512)\n",
    "        self.fc_dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "\n",
    "        # First Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(64 * 50 * 50, 512)\n",
    "        self.fc_dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc_dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(F.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc_dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexCNN(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(ComplexCNN, self).__init__()\n",
    "\n",
    "        # First Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(128 * 50 * 50, 512)\n",
    "        self.fc_dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc_dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(F.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool2(F.relu(self.bn4(self.conv4(x))))\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc_dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_encoded_split, dtype=torch.int64).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val_encoded_split, dtype=torch.int64).to(device)\n",
    "\n",
    "X_test_array = image_data_test.reshape(-1, 1, 200, 200)\n",
    "X_test_tensor = torch.tensor(X_test_array, dtype=torch.float32).to(device)\n",
    "\n",
    "n_classes = full_y_train.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHOOSE MODEL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComplexCNN(output_dim=n_classes) #Choose the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplexCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=320000, out_features=512, bias=True)\n",
      "  (fc_dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc_dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=256, out_features=99, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] - Loss: 4.54317045211792\n",
      "Epoch [2/40] - Loss: 4.515767574310303\n",
      "Epoch [3/40] - Loss: 4.219857215881348\n",
      "Epoch [4/40] - Loss: 3.807262659072876\n",
      "Epoch [5/40] - Loss: 5.171720504760742\n",
      "Epoch [6/40] - Loss: 4.435473442077637\n",
      "Epoch [7/40] - Loss: 4.625889778137207\n",
      "Epoch [8/40] - Loss: 2.704554557800293\n",
      "Epoch [9/40] - Loss: 1.8410496711730957\n",
      "Epoch [10/40] - Loss: 4.022224426269531\n",
      "Epoch [11/40] - Loss: 4.561795234680176\n",
      "Epoch [12/40] - Loss: 5.1313629150390625\n",
      "Epoch [13/40] - Loss: 4.212680816650391\n",
      "Epoch [14/40] - Loss: 2.3781728744506836\n",
      "Epoch [15/40] - Loss: 3.024986982345581\n",
      "Epoch [16/40] - Loss: 4.193324089050293\n",
      "Epoch [17/40] - Loss: 5.149500846862793\n",
      "Epoch [18/40] - Loss: 4.613173484802246\n",
      "Epoch [19/40] - Loss: 2.279524564743042\n",
      "Epoch [20/40] - Loss: 1.5372583866119385\n",
      "Epoch [21/40] - Loss: 2.857595920562744\n",
      "Epoch [22/40] - Loss: 1.5753371715545654\n",
      "Epoch [23/40] - Loss: 2.185105323791504\n",
      "Epoch [24/40] - Loss: 3.041677951812744\n",
      "Epoch [25/40] - Loss: 3.332268714904785\n",
      "Epoch [26/40] - Loss: 2.5915896892547607\n",
      "Epoch [27/40] - Loss: 0.7424209713935852\n",
      "Epoch [28/40] - Loss: 2.1164915561676025\n",
      "Epoch [29/40] - Loss: 2.8156421184539795\n",
      "Epoch [30/40] - Loss: 0.47230371832847595\n",
      "Epoch [31/40] - Loss: 2.546674966812134\n",
      "Epoch [32/40] - Loss: 0.1396622657775879\n",
      "Epoch [33/40] - Loss: 3.033536434173584\n",
      "Epoch [34/40] - Loss: 0.8770791292190552\n",
      "Epoch [35/40] - Loss: 1.1521291732788086\n",
      "Epoch [36/40] - Loss: 0.14277374744415283\n",
      "Epoch [37/40] - Loss: 2.0211687088012695\n",
      "Epoch [38/40] - Loss: 0.14087934792041779\n",
      "Epoch [39/40] - Loss: 0.022349750623106956\n",
      "Epoch [40/40] - Loss: 2.3484199047088623\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 40\n",
    "batch_size = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        # Determine end index for the current batch\n",
    "        end_idx = min(i + batch_size, len(X_train_tensor))\n",
    "        \n",
    "        # Get the mini-batch data\n",
    "        inputs = X_train_tensor[i:end_idx]\n",
    "        labels = y_train_tensor[i:end_idx]\n",
    "\n",
    "        # Ensure the input data has an additional channel dimension (assuming grayscale images)\n",
    "        inputs = inputs.unsqueeze(1)  # Add a channel dimension\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {loss.item()}\")\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation set: 31.82%\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Prevent gradient calculations\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_val_tensor), batch_size):\n",
    "        # Determine end index for the current batch\n",
    "        end_idx = min(i + batch_size, len(X_val_tensor))\n",
    "        \n",
    "        # Get the mini-batch data\n",
    "        inputs = X_val_tensor[i:end_idx]\n",
    "        labels = y_val_tensor[i:end_idx]\n",
    "        \n",
    "        # Ensure the input data has an additional channel dimension (assuming grayscale images)\n",
    "        inputs = inputs.unsqueeze(1)  # Add a channel dimension\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct_predictions / total_predictions\n",
    "print(f'Accuracy on the validation set: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Placeholder for the probabilities\n",
    "test_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_test_tensor), batch_size):\n",
    "        # Get mini-batch\n",
    "        inputs = X_test_tensor[i:i+batch_size]\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Apply softmax to obtain probabilities\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        test_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# Read the sample_submission.csv\n",
    "proba_df = pd.read_csv(\"CSV/sample_submission.csv\")\n",
    "\n",
    "# Replace the data in the columns (excluding the \"id\" column) with the computed probabilities\n",
    "# Ensure that the columns in proba_df (excluding 'id') match the order and number of your model's output classes\n",
    "proba_df.iloc[:, 1:] = test_probabilities\n",
    "\n",
    "# Save to CSV file for submission\n",
    "proba_df.to_csv('CNN_output.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
