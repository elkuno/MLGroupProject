{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with MLP(Multi-Layer Perceptron) on image data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X_train_transformed = pd.read_csv('CSV/pre-processed/full_X_train_transformed')\n",
    "full_y_train = pd.read_csv('CSV/pre-processed/full_y_train')\n",
    "full_y_train = full_y_train.drop(columns=['id'])\n",
    "full_y_train = full_y_train.iloc[:, 0]\n",
    "\n",
    "\n",
    "\n",
    "full_X_test_transformed = pd.read_csv('CSV/pre-processed/full_X_test_transformed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only the image data from the full_X_train_transformed DataFrame\n",
    "image_data_train = full_X_train_transformed.iloc[:, -40000:]\n",
    "image_data_test = full_X_test_transformed.iloc[:, -40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 40000)\n",
      "(594, 40000)\n",
      "(990,)\n"
     ]
    }
   ],
   "source": [
    "print(image_data_train.shape)\n",
    "print(image_data_test.shape)\n",
    "print(full_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_train.columns = range(40000)\n",
    "image_data_test.columns = range(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (792, 40000)\n",
      "Validation set size: (198, 40000)\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(full_y_train)\n",
    "\n",
    "# Splitting 20% of the training data as a validation set\n",
    "X_train, X_val, y_train_encoded_split, y_val_encoded_split = train_test_split(\n",
    "    image_data_train, y_train_encoded, test_size=0.2, stratify=y_train_encoded, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=40000, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=99, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_encoded_split, dtype=torch.int64).to(device)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val_encoded_split, dtype=torch.int64).to(device)\n",
    "\n",
    "X_test_tensor = torch.tensor(image_data_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define the model\n",
    "n_classes = full_y_train.nunique()\n",
    "model = MLP(input_dim=40000, output_dim=n_classes)\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Loss: 4.655791759490967\n",
      "Epoch [2/100] - Loss: 4.598121643066406\n",
      "Epoch [3/100] - Loss: 4.5069661140441895\n",
      "Epoch [4/100] - Loss: 4.515850067138672\n",
      "Epoch [5/100] - Loss: 4.615506172180176\n",
      "Epoch [6/100] - Loss: 4.664968013763428\n",
      "Epoch [7/100] - Loss: 4.564562797546387\n",
      "Epoch [8/100] - Loss: 4.419679641723633\n",
      "Epoch [9/100] - Loss: 4.383508682250977\n",
      "Epoch [10/100] - Loss: 4.086696147918701\n",
      "Epoch [11/100] - Loss: 4.02334451675415\n",
      "Epoch [12/100] - Loss: 3.7132883071899414\n",
      "Epoch [13/100] - Loss: 3.72428035736084\n",
      "Epoch [14/100] - Loss: 3.756972551345825\n",
      "Epoch [15/100] - Loss: 3.4063806533813477\n",
      "Epoch [16/100] - Loss: 3.2324485778808594\n",
      "Epoch [17/100] - Loss: 2.9210243225097656\n",
      "Epoch [18/100] - Loss: 3.2053515911102295\n",
      "Epoch [19/100] - Loss: 3.055867910385132\n",
      "Epoch [20/100] - Loss: 2.6801788806915283\n",
      "Epoch [21/100] - Loss: 2.6555473804473877\n",
      "Epoch [22/100] - Loss: 2.5806970596313477\n",
      "Epoch [23/100] - Loss: 2.439872980117798\n",
      "Epoch [24/100] - Loss: 2.1843314170837402\n",
      "Epoch [25/100] - Loss: 2.268279552459717\n",
      "Epoch [26/100] - Loss: 2.024125576019287\n",
      "Epoch [27/100] - Loss: 2.0628268718719482\n",
      "Epoch [28/100] - Loss: 2.3583168983459473\n",
      "Epoch [29/100] - Loss: 2.0688636302948\n",
      "Epoch [30/100] - Loss: 2.058415174484253\n",
      "Epoch [31/100] - Loss: 1.854169249534607\n",
      "Epoch [32/100] - Loss: 1.9891462326049805\n",
      "Epoch [33/100] - Loss: 2.183480978012085\n",
      "Epoch [34/100] - Loss: 1.8857049942016602\n",
      "Epoch [35/100] - Loss: 1.7433489561080933\n",
      "Epoch [36/100] - Loss: 1.7155002355575562\n",
      "Epoch [37/100] - Loss: 2.0128047466278076\n",
      "Epoch [38/100] - Loss: 1.7934030294418335\n",
      "Epoch [39/100] - Loss: 1.6765556335449219\n",
      "Epoch [40/100] - Loss: 1.3404902219772339\n",
      "Epoch [41/100] - Loss: 1.7584267854690552\n",
      "Epoch [42/100] - Loss: 1.5971378087997437\n",
      "Epoch [43/100] - Loss: 1.4488288164138794\n",
      "Epoch [44/100] - Loss: 1.6573591232299805\n",
      "Epoch [45/100] - Loss: 1.3775585889816284\n",
      "Epoch [46/100] - Loss: 1.4538488388061523\n",
      "Epoch [47/100] - Loss: 1.5731185674667358\n",
      "Epoch [48/100] - Loss: 1.5566962957382202\n",
      "Epoch [49/100] - Loss: 1.2762037515640259\n",
      "Epoch [50/100] - Loss: 1.2855048179626465\n",
      "Epoch [51/100] - Loss: 1.5305861234664917\n",
      "Epoch [52/100] - Loss: 1.2697125673294067\n",
      "Epoch [53/100] - Loss: 1.3495153188705444\n",
      "Epoch [54/100] - Loss: 1.2797577381134033\n",
      "Epoch [55/100] - Loss: 1.1604936122894287\n",
      "Epoch [56/100] - Loss: 1.3219298124313354\n",
      "Epoch [57/100] - Loss: 1.2704377174377441\n",
      "Epoch [58/100] - Loss: 1.2758026123046875\n",
      "Epoch [59/100] - Loss: 1.1391527652740479\n",
      "Epoch [60/100] - Loss: 1.0671323537826538\n",
      "Epoch [61/100] - Loss: 1.2939047813415527\n",
      "Epoch [62/100] - Loss: 1.0652127265930176\n",
      "Epoch [63/100] - Loss: 1.1699315309524536\n",
      "Epoch [64/100] - Loss: 0.9979869723320007\n",
      "Epoch [65/100] - Loss: 1.0248459577560425\n",
      "Epoch [66/100] - Loss: 0.8477765917778015\n",
      "Epoch [67/100] - Loss: 0.8465649485588074\n",
      "Epoch [68/100] - Loss: 1.3811310529708862\n",
      "Epoch [69/100] - Loss: 1.0509637594223022\n",
      "Epoch [70/100] - Loss: 1.012976050376892\n",
      "Epoch [71/100] - Loss: 0.7477843165397644\n",
      "Epoch [72/100] - Loss: 0.7941616177558899\n",
      "Epoch [73/100] - Loss: 0.9003317356109619\n",
      "Epoch [74/100] - Loss: 0.6460265517234802\n",
      "Epoch [75/100] - Loss: 0.6621896028518677\n",
      "Epoch [76/100] - Loss: 0.9076011180877686\n",
      "Epoch [77/100] - Loss: 0.9623963832855225\n",
      "Epoch [78/100] - Loss: 0.8350288271903992\n",
      "Epoch [79/100] - Loss: 0.861108124256134\n",
      "Epoch [80/100] - Loss: 1.1319907903671265\n",
      "Epoch [81/100] - Loss: 0.8716076016426086\n",
      "Epoch [82/100] - Loss: 0.6629611849784851\n",
      "Epoch [83/100] - Loss: 0.6797506809234619\n",
      "Epoch [84/100] - Loss: 0.7453989386558533\n",
      "Epoch [85/100] - Loss: 0.7318644523620605\n",
      "Epoch [86/100] - Loss: 0.6459602117538452\n",
      "Epoch [87/100] - Loss: 0.612766683101654\n",
      "Epoch [88/100] - Loss: 0.7206494212150574\n",
      "Epoch [89/100] - Loss: 0.4613513648509979\n",
      "Epoch [90/100] - Loss: 0.7262282967567444\n",
      "Epoch [91/100] - Loss: 0.7947626709938049\n",
      "Epoch [92/100] - Loss: 0.6211466789245605\n",
      "Epoch [93/100] - Loss: 0.873175323009491\n",
      "Epoch [94/100] - Loss: 0.6372072696685791\n",
      "Epoch [95/100] - Loss: 1.2322227954864502\n",
      "Epoch [96/100] - Loss: 0.6515927314758301\n",
      "Epoch [97/100] - Loss: 0.8914690017700195\n",
      "Epoch [98/100] - Loss: 0.5226700305938721\n",
      "Epoch [99/100] - Loss: 0.5581126809120178\n",
      "Epoch [100/100] - Loss: 0.797680139541626\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        # Determine end index for the current batch\n",
    "        end_idx = min(i + batch_size, len(X_train_tensor))\n",
    "        \n",
    "        # Get the mini-batch data\n",
    "        inputs = X_train_tensor[i:end_idx]\n",
    "        labels = y_train_tensor[i:end_idx]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {loss.item()}\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation set: 55.56%\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Prevent gradient calculations\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_val_tensor), batch_size):\n",
    "        # Determine end index for the current batch\n",
    "        end_idx = min(i + batch_size, len(X_val_tensor))\n",
    "        \n",
    "        # Get the mini-batch data\n",
    "        inputs = X_val_tensor[i:end_idx]\n",
    "        labels = y_val_tensor[i:end_idx]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct_predictions / total_predictions\n",
    "print(f'Accuracy on the validation set: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Placeholder for the probabilities\n",
    "test_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_test_tensor), batch_size):\n",
    "        # Get mini-batch\n",
    "        inputs = X_test_tensor[i:i+batch_size]\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Apply softmax to obtain probabilities\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        test_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# Read the sample_submission.csv\n",
    "proba_df = pd.read_csv(\"CSV/sample_submission.csv\")\n",
    "\n",
    "# Replace the data in the columns (excluding the \"id\" column) with the computed probabilities\n",
    "# Ensure that the columns in proba_df (excluding 'id') match the order and number of your model's output classes\n",
    "proba_df.iloc[:, 1:] = test_probabilities\n",
    "\n",
    "# Save to CSV file for submission\n",
    "proba_df.to_csv('MLP_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "56.06% accuracy on the validation set after 100 epoch training with LR = 0.0001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
