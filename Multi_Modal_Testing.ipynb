{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X_train_transformed = pd.read_csv('CSV/pre-processed/full_X_train_transformed')\n",
    "full_y_train = pd.read_csv('CSV/pre-processed/full_y_train')\n",
    "full_y_train = full_y_train.drop(columns=['id'])\n",
    "full_y_train = full_y_train.iloc[:, 0]\n",
    "\n",
    "full_X_test_transformed = pd.read_csv('CSV/pre-processed/full_X_test_transformed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(full_y_train)\n",
    "\n",
    "# Now, split the combined dataset into training and validation sets\n",
    "X_train_combined, X_val_combined, y_train_encoded_split, y_val_encoded_split = train_test_split(\n",
    "    full_X_train_transformed, y_train_encoded, test_size=0.1, stratify=y_train_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# Extract the image data for the training and validation sets\n",
    "X_train_image = X_train_combined.iloc[:, -40000:].values.reshape(-1, 200, 200)\n",
    "X_val_image = X_val_combined.iloc[:, -40000:].values.reshape(-1, 200, 200)\n",
    "X_test_image = full_X_test_transformed.iloc[:, -40000:].values.reshape(-1, 200, 200)\n",
    "\n",
    "# Convert grayscale images to 3-channel (RGB) format\n",
    "X_train_rgb = np.stack((X_train_image,) * 3, axis=-1)\n",
    "X_val_rgb = np.stack((X_val_image,) * 3, axis=-1)\n",
    "X_test_rgb = np.stack((X_test_image,) * 3, axis=-1)\n",
    "\n",
    "# Convert the RGB numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_rgb.transpose(0, 3, 1, 2), dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val_rgb.transpose(0, 3, 1, 2), dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_rgb.transpose(0, 3, 1, 2), dtype=torch.float32).to(device)\n",
    "\n",
    "# Normalize using ImageNet statistics\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "X_train_normalized = (X_train_tensor - mean) / std\n",
    "X_val_normalized = (X_val_tensor - mean) / std\n",
    "X_test_normalized = (X_test_tensor - mean) / std\n",
    "\n",
    "# Extracting the CSV data for training and validation sets\n",
    "csv_X_train = X_train_combined.iloc[:, :-40000]\n",
    "csv_X_val = X_val_combined.iloc[:, :-40000]\n",
    "csv_X_test = full_X_test_transformed.iloc[:, :-40000]\n",
    "\n",
    "# Convert CSV data to PyTorch tensors and move them to the device\n",
    "csv_X_train_tensor = torch.tensor(csv_X_train.values, dtype=torch.float32).to(device)\n",
    "csv_X_val_tensor = torch.tensor(csv_X_val.values, dtype=torch.float32).to(device)\n",
    "csv_X_test_tensor = torch.tensor(csv_X_test.values, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Image Training tensor shape: torch.Size([792, 3, 200, 200])\n",
      "Normalized Image Validation tensor shape: torch.Size([198, 3, 200, 200])\n",
      "Normalized Image Test tensor shape: torch.Size([594, 3, 200, 200])\n",
      "\n",
      "CSV Training tensor shape: torch.Size([792, 193])\n",
      "CSV Validation tensor shape: torch.Size([198, 193])\n",
      "CSV Test tensor shape: torch.Size([594, 193])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Normalized Image Training tensor shape: {X_train_normalized.shape}\")\n",
    "print(f\"Normalized Image Validation tensor shape: {X_val_normalized.shape}\")\n",
    "print(f\"Normalized Image Test tensor shape: {X_test_normalized.shape}\\n\")\n",
    "\n",
    "print(f\"CSV Training tensor shape: {csv_X_train_tensor.shape}\")\n",
    "print(f\"CSV Validation tensor shape: {csv_X_val_tensor.shape}\")\n",
    "print(f\"CSV Test tensor shape: {csv_X_test_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet152_Weights\n",
    "\n",
    "# Define the model using ResNet152\n",
    "n_classes = full_y_train.nunique()\n",
    "resnet_model = models.resnet152(weights=ResNet152_Weights.IMAGENET1K_V1)  # Load the ResNet152 with ImageNet weights\n",
    "\n",
    "resnet_features = resnet_model.fc.in_features  # Storing the number of features\n",
    "\n",
    "resnet_model.fc = nn.Identity()  # Replacing the final layer\n",
    "\n",
    "csv_input_dim = csv_X_train.shape[1]\n",
    "csv_hidden_dim = 512  # You can adjust this\n",
    "csv_model = nn.Sequential(\n",
    "    nn.Linear(csv_input_dim, csv_hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(csv_hidden_dim, csv_hidden_dim),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, resnet_model, csv_model, output_dim, resnet_features):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.resnet_model = resnet_model\n",
    "        self.csv_model = csv_model\n",
    "        self.fc = nn.Linear(resnet_features + csv_hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, image_data, csv_data):\n",
    "        x1 = self.resnet_model(image_data)\n",
    "        x2 = self.csv_model(csv_data)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = CombinedModel(resnet_model, csv_model, n_classes, resnet_features)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Convert label data to tensors\n",
    "y_train_tensor = torch.tensor(y_train_encoded_split, dtype=torch.int64).to(device)\n",
    "y_val_tensor = torch.tensor(y_val_encoded_split, dtype=torch.int64).to(device)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_normalized, csv_X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_normalized, csv_X_val_tensor, y_val_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CombinedModel(\n",
      "  (resnet_model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (6): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (7): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (6): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (7): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (8): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (9): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (10): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (11): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (12): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (13): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (14): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (15): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (16): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (17): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (18): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (19): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (20): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (21): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (22): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (23): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (24): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (25): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (26): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (27): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (28): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (29): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (30): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (31): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (32): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (33): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (34): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (35): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (csv_model): Sequential(\n",
      "    (0): Linear(in_features=193, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=1024, out_features=768, bias=True)\n",
      "    (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=2560, out_features=99, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.resnet import ResNet152_Weights\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Define the model using ResNet152\n",
    "n_classes = full_y_train.nunique()\n",
    "resnet_model = models.resnet152(weights=ResNet152_Weights.IMAGENET1K_V1)  # Load the ResNet152 with ImageNet weights\n",
    "\n",
    "resnet_features = resnet_model.fc.in_features  # Storing the number of features\n",
    "\n",
    "resnet_model.fc = nn.Identity()  # Replacing the final layer\n",
    "\n",
    "csv_input_dim = csv_X_train.shape[1]\n",
    "csv_hidden_dim1 = 1024  # Increased from 512\n",
    "csv_hidden_dim2 = 768  # New hidden layer\n",
    "csv_hidden_dim3 = 512  # New hidden layer\n",
    "dropout_rate = 0.5     # Dropout rate\n",
    "\n",
    "csv_model = nn.Sequential(\n",
    "    nn.Linear(csv_input_dim, csv_hidden_dim1),\n",
    "    nn.BatchNorm1d(csv_hidden_dim1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout_rate),\n",
    "    \n",
    "    nn.Linear(csv_hidden_dim1, csv_hidden_dim2),\n",
    "    nn.BatchNorm1d(csv_hidden_dim2),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout_rate),\n",
    "    \n",
    "    nn.Linear(csv_hidden_dim2, csv_hidden_dim3),\n",
    "    nn.BatchNorm1d(csv_hidden_dim3),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout_rate),\n",
    "    \n",
    "    nn.Linear(csv_hidden_dim3, csv_hidden_dim3),\n",
    "    nn.BatchNorm1d(csv_hidden_dim3),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout_rate),\n",
    ")\n",
    "\n",
    "learning_rate = 0.00001\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, resnet_model, csv_model, output_dim, resnet_features):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.resnet_model = resnet_model\n",
    "        self.csv_model = csv_model\n",
    "        self.fc = nn.Linear(resnet_features + csv_hidden_dim3, output_dim)  # Ensure correct dimensions\n",
    "        \n",
    "    def forward(self, image_data, csv_data):\n",
    "        x1 = self.resnet_model(image_data)\n",
    "        x2 = self.csv_model(csv_data)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CombinedModel(resnet_model, csv_model, n_classes, resnet_features)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Convert label data to tensors\n",
    "y_train_tensor = torch.tensor(y_train_encoded_split, dtype=torch.int64).to(device)\n",
    "y_val_tensor = torch.tensor(y_val_encoded_split, dtype=torch.int64).to(device)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_normalized, csv_X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_normalized, csv_X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_normalized, csv_X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 1000\n",
    "batch_size = 8\n",
    "patience = 20  # Number of epochs to wait for improvement before stopping\n",
    "\n",
    "# Define StepLR scheduler. Let's say we decrease the learning rate by 0.1 every 30 epochs\n",
    "STEP_SIZE = 40\n",
    "GAMMA = 0.1\n",
    "scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be saved as: ResNet152_CSV_L193-1024-768-512_Dropout50_lr1e-05_SS40_bs8_20231027_1431.pth\n"
     ]
    }
   ],
   "source": [
    "# Extract model details\n",
    "base_name = \"ResNet152\"\n",
    "csv_layers = f\"CSV_L{csv_input_dim}-{csv_hidden_dim1}-{csv_hidden_dim2}-{csv_hidden_dim3}\"  # Model architecture\n",
    "dropout_info = f\"Dropout{int(dropout_rate*100)}\"  # Dropout percentage\n",
    "lr_info = f\"lr{learning_rate}\"\n",
    "\n",
    "# Get the current date and time\n",
    "from datetime import datetime\n",
    "current_datetime = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "\n",
    "# Formulate the name\n",
    "checkpoint_name = f\"{base_name}_{csv_layers}_{dropout_info}_{lr_info}_SS{STEP_SIZE}_bs{batch_size}_{current_datetime}.pth\"\n",
    "checkpoint_path = f\"model_checkpoints/multi_modal/{checkpoint_name}\"\n",
    "\n",
    "print(f\"Model will be saved as: {checkpoint_name}\")\n",
    "\n",
    "# Path to save the best model\n",
    "checkpoint_path = f\"model_checkpoints/{base_name}/{checkpoint_name}.pth\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] - Training Loss: 4.5056, Validation Loss: 4.1764\n",
      "Epoch [2/1000] - Training Loss: 4.0311, Validation Loss: 3.6933\n",
      "Epoch [3/1000] - Training Loss: 3.5990, Validation Loss: 3.2572\n",
      "Epoch [4/1000] - Training Loss: 3.2487, Validation Loss: 2.8810\n",
      "Epoch [5/1000] - Training Loss: 2.8433, Validation Loss: 2.4484\n",
      "Epoch [6/1000] - Training Loss: 2.4886, Validation Loss: 2.1109\n",
      "Epoch [7/1000] - Training Loss: 2.1792, Validation Loss: 1.8121\n",
      "Epoch [8/1000] - Training Loss: 1.8872, Validation Loss: 1.5120\n",
      "Epoch [9/1000] - Training Loss: 1.6118, Validation Loss: 1.2696\n",
      "Epoch [10/1000] - Training Loss: 1.3768, Validation Loss: 1.0940\n",
      "Epoch [11/1000] - Training Loss: 1.1496, Validation Loss: 0.9017\n",
      "Epoch [12/1000] - Training Loss: 0.9593, Validation Loss: 0.8034\n",
      "Epoch [13/1000] - Training Loss: 0.8129, Validation Loss: 0.6377\n",
      "Epoch [14/1000] - Training Loss: 0.6830, Validation Loss: 0.5812\n",
      "Epoch [15/1000] - Training Loss: 0.5485, Validation Loss: 0.4984\n",
      "Epoch [16/1000] - Training Loss: 0.4583, Validation Loss: 0.4420\n",
      "Epoch [17/1000] - Training Loss: 0.3876, Validation Loss: 0.4092\n",
      "Epoch [18/1000] - Training Loss: 0.3246, Validation Loss: 0.3472\n",
      "Epoch [19/1000] - Training Loss: 0.2719, Validation Loss: 0.3087\n",
      "Epoch [20/1000] - Training Loss: 0.2438, Validation Loss: 0.3038\n",
      "Epoch [21/1000] - Training Loss: 0.2081, Validation Loss: 0.2891\n",
      "Epoch [22/1000] - Training Loss: 0.1855, Validation Loss: 0.2522\n",
      "Epoch [23/1000] - Training Loss: 0.1664, Validation Loss: 0.2910\n",
      "Epoch [24/1000] - Training Loss: 0.1506, Validation Loss: 0.2478\n",
      "Epoch [25/1000] - Training Loss: 0.1370, Validation Loss: 0.2311\n",
      "Epoch [26/1000] - Training Loss: 0.1211, Validation Loss: 0.2181\n",
      "Epoch [27/1000] - Training Loss: 0.0994, Validation Loss: 0.2262\n",
      "Epoch [28/1000] - Training Loss: 0.0901, Validation Loss: 0.1998\n",
      "Epoch [29/1000] - Training Loss: 0.0846, Validation Loss: 0.2173\n",
      "Epoch [30/1000] - Training Loss: 0.0845, Validation Loss: 0.2231\n",
      "Epoch [31/1000] - Training Loss: 0.0717, Validation Loss: 0.2001\n",
      "Epoch [32/1000] - Training Loss: 0.0732, Validation Loss: 0.2223\n",
      "Epoch [33/1000] - Training Loss: 0.0621, Validation Loss: 0.2254\n",
      "Epoch [34/1000] - Training Loss: 0.0621, Validation Loss: 0.1844\n",
      "Epoch [35/1000] - Training Loss: 0.0562, Validation Loss: 0.1934\n",
      "Epoch [36/1000] - Training Loss: 0.0623, Validation Loss: 0.2425\n",
      "Epoch [37/1000] - Training Loss: 0.0539, Validation Loss: 0.1805\n",
      "Epoch [38/1000] - Training Loss: 0.0423, Validation Loss: 0.1737\n",
      "Epoch [39/1000] - Training Loss: 0.0412, Validation Loss: 0.1773\n",
      "Epoch [40/1000] - Training Loss: 0.0432, Validation Loss: 0.1960\n",
      "Epoch [41/1000] - Training Loss: 0.0384, Validation Loss: 0.1628\n",
      "Epoch [42/1000] - Training Loss: 0.0384, Validation Loss: 0.1874\n",
      "Epoch [43/1000] - Training Loss: 0.0363, Validation Loss: 0.1725\n",
      "Epoch [44/1000] - Training Loss: 0.0348, Validation Loss: 0.1816\n",
      "Epoch [45/1000] - Training Loss: 0.0324, Validation Loss: 0.1609\n",
      "Epoch [46/1000] - Training Loss: 0.0315, Validation Loss: 0.1644\n",
      "Epoch [47/1000] - Training Loss: 0.0354, Validation Loss: 0.1779\n",
      "Epoch [48/1000] - Training Loss: 0.0301, Validation Loss: 0.1760\n",
      "Epoch [49/1000] - Training Loss: 0.0292, Validation Loss: 0.1712\n",
      "Epoch [50/1000] - Training Loss: 0.0386, Validation Loss: 0.1706\n",
      "Epoch [51/1000] - Training Loss: 0.0289, Validation Loss: 0.1473\n",
      "Epoch [52/1000] - Training Loss: 0.0298, Validation Loss: 0.1810\n",
      "Epoch [53/1000] - Training Loss: 0.0321, Validation Loss: 0.1553\n",
      "Epoch [54/1000] - Training Loss: 0.0313, Validation Loss: 0.1789\n",
      "Epoch [55/1000] - Training Loss: 0.0321, Validation Loss: 0.1532\n",
      "Epoch [56/1000] - Training Loss: 0.0308, Validation Loss: 0.1748\n",
      "Epoch [57/1000] - Training Loss: 0.0286, Validation Loss: 0.1413\n",
      "Epoch [58/1000] - Training Loss: 0.0298, Validation Loss: 0.1695\n",
      "Epoch [59/1000] - Training Loss: 0.0258, Validation Loss: 0.1790\n",
      "Epoch [60/1000] - Training Loss: 0.0295, Validation Loss: 0.1688\n",
      "Epoch [61/1000] - Training Loss: 0.0252, Validation Loss: 0.1640\n",
      "Epoch [62/1000] - Training Loss: 0.0294, Validation Loss: 0.1602\n",
      "Epoch [63/1000] - Training Loss: 0.0251, Validation Loss: 0.1493\n",
      "Epoch [64/1000] - Training Loss: 0.0278, Validation Loss: 0.1536\n",
      "Epoch [65/1000] - Training Loss: 0.0298, Validation Loss: 0.1372\n",
      "Epoch [66/1000] - Training Loss: 0.0256, Validation Loss: 0.1481\n",
      "Epoch [67/1000] - Training Loss: 0.0309, Validation Loss: 0.1364\n",
      "Epoch [68/1000] - Training Loss: 0.0243, Validation Loss: 0.1605\n",
      "Epoch [69/1000] - Training Loss: 0.0232, Validation Loss: 0.1704\n",
      "Epoch [70/1000] - Training Loss: 0.0234, Validation Loss: 0.1772\n",
      "Epoch [71/1000] - Training Loss: 0.0253, Validation Loss: 0.1691\n",
      "Epoch [72/1000] - Training Loss: 0.0260, Validation Loss: 0.1641\n",
      "Epoch [73/1000] - Training Loss: 0.0238, Validation Loss: 0.1380\n",
      "Epoch [74/1000] - Training Loss: 0.0219, Validation Loss: 0.1252\n",
      "Epoch [75/1000] - Training Loss: 0.0232, Validation Loss: 0.1434\n",
      "Epoch [76/1000] - Training Loss: 0.0210, Validation Loss: 0.1637\n",
      "Epoch [77/1000] - Training Loss: 0.0216, Validation Loss: 0.1584\n",
      "Epoch [78/1000] - Training Loss: 0.0207, Validation Loss: 0.1589\n",
      "Epoch [79/1000] - Training Loss: 0.0247, Validation Loss: 0.1430\n",
      "Epoch [80/1000] - Training Loss: 0.0235, Validation Loss: 0.1375\n",
      "Epoch [81/1000] - Training Loss: 0.0206, Validation Loss: 0.1312\n",
      "Epoch [82/1000] - Training Loss: 0.0219, Validation Loss: 0.1447\n",
      "Epoch [83/1000] - Training Loss: 0.0200, Validation Loss: 0.1331\n",
      "Epoch [84/1000] - Training Loss: 0.0246, Validation Loss: 0.1733\n",
      "Epoch [85/1000] - Training Loss: 0.0206, Validation Loss: 0.1410\n",
      "Epoch [86/1000] - Training Loss: 0.0269, Validation Loss: 0.1227\n",
      "Epoch [87/1000] - Training Loss: 0.0193, Validation Loss: 0.1457\n",
      "Epoch [88/1000] - Training Loss: 0.0212, Validation Loss: 0.1425\n",
      "Epoch [89/1000] - Training Loss: 0.0249, Validation Loss: 0.1393\n",
      "Epoch [90/1000] - Training Loss: 0.0194, Validation Loss: 0.1639\n",
      "Epoch [91/1000] - Training Loss: 0.0198, Validation Loss: 0.1499\n",
      "Epoch [92/1000] - Training Loss: 0.0171, Validation Loss: 0.1213\n",
      "Epoch [93/1000] - Training Loss: 0.0236, Validation Loss: 0.1438\n",
      "Epoch [94/1000] - Training Loss: 0.0198, Validation Loss: 0.1457\n",
      "Epoch [95/1000] - Training Loss: 0.0196, Validation Loss: 0.1238\n",
      "Epoch [96/1000] - Training Loss: 0.0188, Validation Loss: 0.1455\n",
      "Epoch [97/1000] - Training Loss: 0.0182, Validation Loss: 0.1422\n",
      "Epoch [98/1000] - Training Loss: 0.0214, Validation Loss: 0.1538\n",
      "Epoch [99/1000] - Training Loss: 0.0190, Validation Loss: 0.1559\n",
      "Epoch [100/1000] - Training Loss: 0.0312, Validation Loss: 0.1521\n",
      "Epoch [101/1000] - Training Loss: 0.0207, Validation Loss: 0.1411\n",
      "Epoch [102/1000] - Training Loss: 0.0232, Validation Loss: 0.1572\n",
      "Epoch [103/1000] - Training Loss: 0.0207, Validation Loss: 0.1397\n",
      "Epoch [104/1000] - Training Loss: 0.0199, Validation Loss: 0.1630\n",
      "Epoch [105/1000] - Training Loss: 0.0231, Validation Loss: 0.1330\n",
      "Epoch [106/1000] - Training Loss: 0.0177, Validation Loss: 0.1355\n",
      "Epoch [107/1000] - Training Loss: 0.0183, Validation Loss: 0.1411\n",
      "Epoch [108/1000] - Training Loss: 0.0204, Validation Loss: 0.1413\n",
      "Epoch [109/1000] - Training Loss: 0.0233, Validation Loss: 0.1627\n",
      "Epoch [110/1000] - Training Loss: 0.0168, Validation Loss: 0.1545\n",
      "Epoch [111/1000] - Training Loss: 0.0182, Validation Loss: 0.1528\n",
      "Epoch [112/1000] - Training Loss: 0.0218, Validation Loss: 0.1612\n",
      "Early stopping after 112 epochs!\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')  # Initialize with a high value\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for image_inputs, csv_inputs, labels in train_loader:\n",
    "        # Move inputs and labels to device\n",
    "        image_inputs, csv_inputs, labels = image_inputs.to(device), csv_inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(image_inputs, csv_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * image_inputs.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Compute validation loss\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for image_inputs, csv_inputs, labels in val_loader:\n",
    "            image_inputs, csv_inputs, labels = image_inputs.to(device), csv_inputs.to(device), labels.to(device)\n",
    "            outputs = model(image_inputs, csv_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * image_inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        # Save the best model weights\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        \n",
    "    # Print statistics\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping after {epoch+1} epochs!\")\n",
    "        break\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation set: 95.96%\n"
     ]
    }
   ],
   "source": [
    "# load the best model\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "# Update the DataLoader to include both image and CSV tensors\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image_inputs, csv_inputs, labels in val_loader:\n",
    "        image_inputs, csv_inputs, labels = image_inputs.to(device), csv_inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass with both image and CSV data\n",
    "        outputs = model(image_inputs, csv_inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct_predictions / total_predictions\n",
    "print(f'Accuracy on the validation set: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters\n",
    "save_path = f'models/{base_name}/{accuracy:.2f}_{checkpoint_name}'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### outputing predictions to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models\\ResNet152\\95.96_ResNet152_CSV_L193-1024-768-512_Dropout50_lr1e-05_SS40_bs8_20231027_1431.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for image_inputs, csv_inputs in test_loader:\n",
    "        image_inputs, csv_inputs = image_inputs.to(device), csv_inputs.to(device)\n",
    "        \n",
    "        # Get the logits from the model\n",
    "        logits = model(image_inputs, csv_inputs)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "# Concatenate all probabilities\n",
    "all_probs_np = np.concatenate(all_probs, axis=0)\n",
    "\n",
    "# Read the sample_submission.csv\n",
    "proba_df = pd.read_csv(\"CSV/sample_submission.csv\")\n",
    "\n",
    "# Replace the data in the columns (excluding the \"id\" column) with the computed probabilities\n",
    "proba_df.iloc[:, 1:] = all_probs_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_df.to_csv('checkpoint2(2).csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
