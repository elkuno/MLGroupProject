{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, Normalizer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMAGE_DIM = 200, 200    # TODO hard-coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training and test data\n",
    "train_df = pd.read_csv(\"train.csv\", index_col='id')\n",
    "test_df = pd.read_csv(\"test.csv\", index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the type of each column for us to know what preprocessing needs to be done to each column\n",
    "numeric_features = [i for i in train_df.drop(columns='species').columns]\n",
    "binary_features = []\n",
    "\n",
    "# Loads an image and applies some basic preprocessing\n",
    "# to it (resizing, black and white)\n",
    "def load_image(filename):\n",
    "    return Image.open(filename) \\\n",
    "                .resize(IMAGE_DIM) \\\n",
    "                .convert(\n",
    "                    mode='1',   # black and white\n",
    "                    dither=Image.Dither.NONE,\n",
    "                )\n",
    "\n",
    "# Converts a Pillow image to a 1D numpy array of pixel data\n",
    "def image_to_flat_array(img):\n",
    "    return np.array(img).reshape((-1))\n",
    "\n",
    "# Loads images with specified indices into one-dimensional\n",
    "# pixel data and concatenates it all into a single dataframe\n",
    "def images_to_df(indices):\n",
    "    # TODO filepath hard-coded, change if needed\n",
    "    imgs = [\n",
    "        load_image(f'images/{i}.jpg') for i in indices\n",
    "    ]\n",
    "\n",
    "    # Converts each image to flat 1D representation\n",
    "    df = pd.DataFrame(\n",
    "               np.asarray(imgs).reshape((len(imgs), -1))\n",
    "           ).set_index(indices)\n",
    "    df.columns = df.columns.astype(str)     # Prevents some obscure errors later\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data corresponding to each dataset\n",
    "img_train_df = images_to_df(train_df.index)\n",
    "img_test_df = images_to_df(test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train_df = img_train_df.astype(int)\n",
    "img_test_df = img_test_df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the img_train columns to the preprocessor lists\n",
    "passthrough_features = [i for i in img_train_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipelines for each type of data we have\n",
    "numeric_pipeline = make_pipeline(\n",
    "    # Normalize all of them to unit norm\n",
    "    Normalizer()\n",
    ")\n",
    "\n",
    "# Define our column transformer/preprocessor itself\n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_pipeline, numeric_features),\n",
    "    ('passthrough', passthrough_features)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenated Dataframes & Column Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenates the image data with the metadata into one dataframe\n",
    "full_train_df = pd.concat([train_df, img_train_df], axis=1)\n",
    "full_test_df = pd.concat([test_df, img_test_df], axis=1)\n",
    "\n",
    "# We want to split X (big X because it is a matrix) and y from each other\n",
    "full_X_train = full_train_df.drop(columns=['species'])\n",
    "full_y_train = full_train_df['species']\n",
    "\n",
    "# Funnily enough, the test has no ground truth...\n",
    "full_X_test = full_test_df\n",
    "\n",
    "# Now we want to fit our preprocessor onto our data, so we can actually transform it (then cast it to a DF)\n",
    "full_X_train_transformed = pd.DataFrame(preprocessor.fit_transform(full_X_train))\n",
    "full_X_test_transformed = pd.DataFrame(preprocessor.transform(full_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_X_train_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a pipeline for all preprocessing and processing\n",
    "np.random.seed()\n",
    "\n",
    "full_pipeline = make_pipeline(\n",
    "    preprocessor,\n",
    "    DummyClassifier(\n",
    "        strategy = \"uniform\",\n",
    "        random_state = np.random.randint(0, 256)\n",
    "    )\n",
    ")\n",
    "\n",
    "# cv_scores = cross_val_score(full_pipeline, full_X_train_transformed, full_y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy classifer that randomly guesses the species a leaf belongs to\n",
    "np.random.seed()\n",
    "dummy = DummyClassifier(\n",
    "    strategy=\"uniform\",\n",
    "    random_state=np.random.randint(0, 256)\n",
    ")\n",
    "dummy.fit(full_X_train_transformed, full_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(dummy, full_X_train_transformed, full_y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for each class\n",
    "proba = pd.DataFrame(\n",
    "    dummy.predict_proba(full_X_test_transformed),\n",
    "    columns=dummy.classes_,\n",
    "    index=test_df.index   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to CSV for submission\n",
    "proba.to_csv('checkpoint1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with DL model on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only the image data from the full_X_train_transformed DataFrame\n",
    "image_data_train = full_X_train_transformed.iloc[:, -40000:]\n",
    "image_data_test = full_X_test_transformed.iloc[:, -40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_data_train.shape)\n",
    "print(image_data_test.shape)\n",
    "print(full_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_train.columns = range(40000)\n",
    "image_data_test.columns = range(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(full_y_train)\n",
    "\n",
    "# Splitting 20% of the training data as a validation set\n",
    "X_train, X_val, y_train_encoded_split, y_val_encoded_split = train_test_split(\n",
    "    image_data_train, y_train_encoded, test_size=0.2, stratify=y_train_encoded, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaN values in X_train:\", X_train.isnull().sum().sum())\n",
    "print(\"NaN values in y_train:\", np.isnan(y_train_encoded_split).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_encoded_split, dtype=torch.int64)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_encoded_split, dtype=torch.int64)\n",
    "\n",
    "# Define the model\n",
    "n_classes = full_y_train.nunique()\n",
    "model = MLP(input_dim=40000, output_dim=n_classes)\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        # Determine end index for the current batch\n",
    "        end_idx = min(i + batch_size, len(X_train_tensor))\n",
    "        \n",
    "        # Get the mini-batch data\n",
    "        inputs = X_train_tensor[i:end_idx]\n",
    "        labels = y_train_tensor[i:end_idx]\n",
    "\n",
    "        # print(\"Inputs shape:\", inputs.shape)\n",
    "        # print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {loss.item()}\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Prevent gradient calculations\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_val_tensor), batch_size):\n",
    "        # Determine end index for the current batch\n",
    "        end_idx = min(i + batch_size, len(X_val_tensor))\n",
    "        \n",
    "        # Get the mini-batch data\n",
    "        inputs = X_val_tensor[i:end_idx]\n",
    "        labels = y_val_tensor[i:end_idx]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct_predictions / total_predictions\n",
    "print(f'Accuracy on the validation set: {accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
